{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-2-12-Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadagarcia/tinyML/blob/main/notebooks/2_2_12_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfcfRz6-jBIb"
      },
      "source": [
        "# Exploring DNN learning with TensorFlow\n",
        "\n",
        "In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test!\n",
        "\n",
        "We start by setting up the problem for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Xd2A45ohcc"
      },
      "source": [
        "# Imports\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3KzJyjv3rnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c776ef-c975-4e31-f4a7-3d920c6a3f27"
      },
      "source": [
        "# Load in fashion MNIST\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Define the base model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)), \n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUkuiQRWkfMz",
        "outputId": "f69f3925-ae4a-4a16-9d35-41c21a06c150"
      },
      "source": [
        "training_images[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzxgpZL1jBIk"
      },
      "source": [
        "Neural Networks learn the best when the data is scaled / normalized to fall in a constant range. One practitioners often use is the range [0,1]. How might you do this to the training and test images used here?\n",
        "\n",
        "*A hint: these images are saved in the standard [RGB](https://www.rapidtables.com/web/color/RGB_Color.html) format*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89vSsfcajBIk"
      },
      "source": [
        "training_images  = training_images / 255\n",
        "test_images = test_images / 255"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca3RYbVtlHas",
        "outputId": "398ea4a4-f0db-4904-f6e1-1ca98642e4c5"
      },
      "source": [
        "training_images[0], training_labels[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "         0.05098039, 0.28627451, 0.        , 0.        , 0.00392157,\n",
              "         0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.00392157, 0.00392157, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.01176471, 0.        , 0.14117647,\n",
              "         0.53333333, 0.49803922, 0.24313725, 0.21176471, 0.        ,\n",
              "         0.        , 0.        , 0.00392157, 0.01176471, 0.01568627,\n",
              "         0.        , 0.        , 0.01176471],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "         0.8       , 0.69019608, 0.5254902 , 0.56470588, 0.48235294,\n",
              "         0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.04705882, 0.03921569, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.60784314,\n",
              "         0.9254902 , 0.81176471, 0.69803922, 0.41960784, 0.61176471,\n",
              "         0.63137255, 0.42745098, 0.25098039, 0.09019608, 0.30196078,\n",
              "         0.50980392, 0.28235294, 0.05882353],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.00392157, 0.        , 0.27058824, 0.81176471,\n",
              "         0.8745098 , 0.85490196, 0.84705882, 0.84705882, 0.63921569,\n",
              "         0.49803922, 0.4745098 , 0.47843137, 0.57254902, 0.55294118,\n",
              "         0.34509804, 0.6745098 , 0.25882353],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "         0.00392157, 0.00392157, 0.        , 0.78431373, 0.90980392,\n",
              "         0.90980392, 0.91372549, 0.89803922, 0.8745098 , 0.8745098 ,\n",
              "         0.84313725, 0.83529412, 0.64313725, 0.49803922, 0.48235294,\n",
              "         0.76862745, 0.89803922, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.71764706, 0.88235294,\n",
              "         0.84705882, 0.8745098 , 0.89411765, 0.92156863, 0.89019608,\n",
              "         0.87843137, 0.87058824, 0.87843137, 0.86666667, 0.8745098 ,\n",
              "         0.96078431, 0.67843137, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.75686275, 0.89411765,\n",
              "         0.85490196, 0.83529412, 0.77647059, 0.70588235, 0.83137255,\n",
              "         0.82352941, 0.82745098, 0.83529412, 0.8745098 , 0.8627451 ,\n",
              "         0.95294118, 0.79215686, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "         0.01176471, 0.        , 0.04705882, 0.85882353, 0.8627451 ,\n",
              "         0.83137255, 0.85490196, 0.75294118, 0.6627451 , 0.89019608,\n",
              "         0.81568627, 0.85490196, 0.87843137, 0.83137255, 0.88627451,\n",
              "         0.77254902, 0.81960784, 0.20392157],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.02352941, 0.        , 0.38823529, 0.95686275, 0.87058824,\n",
              "         0.8627451 , 0.85490196, 0.79607843, 0.77647059, 0.86666667,\n",
              "         0.84313725, 0.83529412, 0.87058824, 0.8627451 , 0.96078431,\n",
              "         0.46666667, 0.65490196, 0.21960784],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.01568627,\n",
              "         0.        , 0.        , 0.21568627, 0.9254902 , 0.89411765,\n",
              "         0.90196078, 0.89411765, 0.94117647, 0.90980392, 0.83529412,\n",
              "         0.85490196, 0.8745098 , 0.91764706, 0.85098039, 0.85098039,\n",
              "         0.81960784, 0.36078431, 0.        ],\n",
              "        [0.        , 0.        , 0.00392157, 0.01568627, 0.02352941,\n",
              "         0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.92941176, 0.88627451, 0.85098039,\n",
              "         0.8745098 , 0.87058824, 0.85882353, 0.87058824, 0.86666667,\n",
              "         0.84705882, 0.8745098 , 0.89803922, 0.84313725, 0.85490196,\n",
              "         1.        , 0.30196078, 0.        ],\n",
              "        [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.24313725,\n",
              "         0.56862745, 0.8       , 0.89411765, 0.81176471, 0.83529412,\n",
              "         0.86666667, 0.85490196, 0.81568627, 0.82745098, 0.85490196,\n",
              "         0.87843137, 0.8745098 , 0.85882353, 0.84313725, 0.87843137,\n",
              "         0.95686275, 0.62352941, 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "         0.17254902, 0.32156863, 0.41960784, 0.74117647, 0.89411765,\n",
              "         0.8627451 , 0.87058824, 0.85098039, 0.88627451, 0.78431373,\n",
              "         0.80392157, 0.82745098, 0.90196078, 0.87843137, 0.91764706,\n",
              "         0.69019608, 0.7372549 , 0.98039216, 0.97254902, 0.91372549,\n",
              "         0.93333333, 0.84313725, 0.        ],\n",
              "        [0.        , 0.22352941, 0.73333333, 0.81568627, 0.87843137,\n",
              "         0.86666667, 0.87843137, 0.81568627, 0.8       , 0.83921569,\n",
              "         0.81568627, 0.81960784, 0.78431373, 0.62352941, 0.96078431,\n",
              "         0.75686275, 0.80784314, 0.8745098 , 1.        , 1.        ,\n",
              "         0.86666667, 0.91764706, 0.86666667, 0.82745098, 0.8627451 ,\n",
              "         0.90980392, 0.96470588, 0.        ],\n",
              "        [0.01176471, 0.79215686, 0.89411765, 0.87843137, 0.86666667,\n",
              "         0.82745098, 0.82745098, 0.83921569, 0.80392157, 0.80392157,\n",
              "         0.80392157, 0.8627451 , 0.94117647, 0.31372549, 0.58823529,\n",
              "         1.        , 0.89803922, 0.86666667, 0.7372549 , 0.60392157,\n",
              "         0.74901961, 0.82352941, 0.8       , 0.81960784, 0.87058824,\n",
              "         0.89411765, 0.88235294, 0.        ],\n",
              "        [0.38431373, 0.91372549, 0.77647059, 0.82352941, 0.87058824,\n",
              "         0.89803922, 0.89803922, 0.91764706, 0.97647059, 0.8627451 ,\n",
              "         0.76078431, 0.84313725, 0.85098039, 0.94509804, 0.25490196,\n",
              "         0.28627451, 0.41568627, 0.45882353, 0.65882353, 0.85882353,\n",
              "         0.86666667, 0.84313725, 0.85098039, 0.8745098 , 0.8745098 ,\n",
              "         0.87843137, 0.89803922, 0.11372549],\n",
              "        [0.29411765, 0.8       , 0.83137255, 0.8       , 0.75686275,\n",
              "         0.80392157, 0.82745098, 0.88235294, 0.84705882, 0.7254902 ,\n",
              "         0.77254902, 0.80784314, 0.77647059, 0.83529412, 0.94117647,\n",
              "         0.76470588, 0.89019608, 0.96078431, 0.9372549 , 0.8745098 ,\n",
              "         0.85490196, 0.83137255, 0.81960784, 0.87058824, 0.8627451 ,\n",
              "         0.86666667, 0.90196078, 0.2627451 ],\n",
              "        [0.18823529, 0.79607843, 0.71764706, 0.76078431, 0.83529412,\n",
              "         0.77254902, 0.7254902 , 0.74509804, 0.76078431, 0.75294118,\n",
              "         0.79215686, 0.83921569, 0.85882353, 0.86666667, 0.8627451 ,\n",
              "         0.9254902 , 0.88235294, 0.84705882, 0.78039216, 0.80784314,\n",
              "         0.72941176, 0.70980392, 0.69411765, 0.6745098 , 0.70980392,\n",
              "         0.80392157, 0.80784314, 0.45098039],\n",
              "        [0.        , 0.47843137, 0.85882353, 0.75686275, 0.70196078,\n",
              "         0.67058824, 0.71764706, 0.76862745, 0.8       , 0.82352941,\n",
              "         0.83529412, 0.81176471, 0.82745098, 0.82352941, 0.78431373,\n",
              "         0.76862745, 0.76078431, 0.74901961, 0.76470588, 0.74901961,\n",
              "         0.77647059, 0.75294118, 0.69019608, 0.61176471, 0.65490196,\n",
              "         0.69411765, 0.82352941, 0.36078431],\n",
              "        [0.        , 0.        , 0.29019608, 0.74117647, 0.83137255,\n",
              "         0.74901961, 0.68627451, 0.6745098 , 0.68627451, 0.70980392,\n",
              "         0.7254902 , 0.7372549 , 0.74117647, 0.7372549 , 0.75686275,\n",
              "         0.77647059, 0.8       , 0.81960784, 0.82352941, 0.82352941,\n",
              "         0.82745098, 0.7372549 , 0.7372549 , 0.76078431, 0.75294118,\n",
              "         0.84705882, 0.66666667, 0.        ],\n",
              "        [0.00784314, 0.        , 0.        , 0.        , 0.25882353,\n",
              "         0.78431373, 0.87058824, 0.92941176, 0.9372549 , 0.94901961,\n",
              "         0.96470588, 0.95294118, 0.95686275, 0.86666667, 0.8627451 ,\n",
              "         0.75686275, 0.74901961, 0.70196078, 0.71372549, 0.71372549,\n",
              "         0.70980392, 0.69019608, 0.65098039, 0.65882353, 0.38823529,\n",
              "         0.22745098, 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "         0.28235294, 0.16078431, 0.1372549 , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        ]]), 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "qodesMJCoocl",
        "outputId": "5ac3a0d8-30bb-4693-c779-6be6e9f83bb2"
      },
      "source": [
        "plt.imshow(training_images[0], cmap='gray')\r\n",
        "print('Label: ', training_labels[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAR1klEQVR4nO3db2yVdZYH8O+xgNqCBaxA+RPBESOTjVvWikbRjI4Q9IUwanB4scGo24kZk5lkTNa4L8bEFxLdmcm+IJN01AyzzjqZZCBi/DcMmcTdFEcqYdtKd0ZACK2lBUFoS6EUzr7og+lgn3Pqfe69z5Xz/SSk7T393fvrvf1yb+95fs9PVBVEdOm7LO8JEFF5MOxEQTDsREEw7ERBMOxEQUwq542JCN/6JyoxVZXxLs/0zC4iq0TkryKyV0SeyXJdRFRaUmifXUSqAPwNwAoAXQB2AlinqnuMMXxmJyqxUjyzLwOwV1X3q+owgN8BWJ3h+oiohLKEfR6AQ2O+7kou+zsi0iQirSLSmuG2iCijkr9Bp6rNAJoBvownylOWZ/ZuAAvGfD0/uYyIKlCWsO8EsFhEFonIFADfB7C1ONMiomIr+GW8qo6IyFMA3gNQBeBVVf24aDMjoqIquPVW0I3xb3aikivJQTVE9M3BsBMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcGwEwVR1lNJU/mJjLsA6ktZVz1OmzbNrC9fvjy19s4772S6be9nq6qqSq2NjIxkuu2svLlbCn3M+MxOFATDThQEw04UBMNOFATDThQEw04UBMNOFAT77Je4yy6z/z8/d+6cWb/++uvN+hNPPGHWh4aGUmuDg4Pm2NOnT5v1Dz/80Kxn6aV7fXDvfvXGZ5mbdfyA9XjymZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCPbZL3FWTxbw++z33HOPWb/33nvNeldXV2rt8ssvN8dWV1eb9RUrVpj1l19+ObXW29trjvXWjHv3m2fq1KmptfPnz5tjT506VdBtZgq7iBwA0A/gHIARVW3Mcn1EVDrFeGa/W1WPFuF6iKiE+Dc7URBZw64A/igiH4lI03jfICJNItIqIq0Zb4uIMsj6Mn65qnaLyCwA20Tk/1T1/bHfoKrNAJoBQESynd2QiAqW6ZldVbuTj30AtgBYVoxJEVHxFRx2EakRkWkXPgewEkBHsSZGRMWV5WX8bABbknW7kwD8l6q+W5RZUdEMDw9nGn/LLbeY9YULF5p1q8/vrQl/7733zPrSpUvN+osvvphaa22130Jqb283652dnWZ92TL7Ra51v7a0tJhjd+zYkVobGBhIrRUcdlXdD+AfCx1PROXF1htREAw7URAMO1EQDDtREAw7URCSdcver3VjPIKuJKzTFnuPr7dM1GpfAcD06dPN+tmzZ1Nr3lJOz86dO8363r17U2tZW5L19fVm3fq5AXvuDz/8sDl248aNqbXW1lacPHly3F8IPrMTBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcE+ewXwtvfNwnt8P/jgA7PuLWH1WD+bt21x1l64teWz1+PftWuXWbd6+ID/s61atSq1dt1115lj582bZ9ZVlX12osgYdqIgGHaiIBh2oiAYdqIgGHaiIBh2oiC4ZXMFKOexDhc7fvy4WffWbQ8NDZl1a1vmSZPsXz9rW2PA7qMDwJVXXpla8/rsd955p1m//fbbzbp3muxZs2al1t59tzRnZOczO1EQDDtREAw7URAMO1EQDDtREAw7URAMO1EQ7LMHV11dbda9frFXP3XqVGrtxIkT5tjPP//crHtr7a3jF7xzCHg/l3e/nTt3zqxbff4FCxaYYwvlPrOLyKsi0iciHWMumyki20Tkk+TjjJLMjoiKZiIv438N4OLTajwDYLuqLgawPfmaiCqYG3ZVfR/AsYsuXg1gU/L5JgBrijwvIiqyQv9mn62qPcnnhwHMTvtGEWkC0FTg7RBRkWR+g05V1TqRpKo2A2gGeMJJojwV2nrrFZF6AEg+9hVvSkRUCoWGfSuA9cnn6wG8UZzpEFGpuC/jReR1AN8BUCciXQB+CmADgN+LyOMADgJYW8pJXuqy9nytnq63Jnzu3Llm/cyZM5nq1np277zwVo8e8PeGt/r0Xp98ypQpZr2/v9+s19bWmvW2trbUmveYNTY2ptb27NmTWnPDrqrrUkrf9cYSUeXg4bJEQTDsREEw7ERBMOxEQTDsREFwiWsF8E4lXVVVZdat1tsjjzxijp0zZ45ZP3LkiFm3TtcM2Es5a2pqzLHeUk+vdWe1/c6ePWuO9U5z7f3cV199tVnfuHFjaq2hocEca83NauPymZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCCnndsE8U834vJ7uyMhIwdd96623mvW33nrLrHtbMmc5BmDatGnmWG9LZu9U05MnTy6oBvjHAHhbXXusn+2ll14yx7722mtmXVXHbbbzmZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oiG/UenZrra7X7/VOx+ydztla/2yt2Z6ILH10z9tvv23WBwcHzbrXZ/dOuWwdx+Gtlfce0yuuuMKse2vWs4z1HnNv7jfddFNqzdvKulB8ZicKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKoqL67FnWRpeyV11qd911l1l/6KGHzPodd9yRWvO2PfbWhHt9dG8tvvWYeXPzfh+s88IDdh/eO4+DNzePd78NDAyk1h588EFz7JtvvlnQnNxndhF5VUT6RKRjzGXPiUi3iOxO/t1f0K0TUdlM5GX8rwGsGufyX6hqQ/LPPkyLiHLnhl1V3wdwrAxzIaISyvIG3VMi0pa8zJ+R9k0i0iQirSLSmuG2iCijQsP+SwDfAtAAoAfAz9K+UVWbVbVRVRsLvC0iKoKCwq6qvap6TlXPA/gVgGXFnRYRFVtBYReR+jFffg9AR9r3ElFlcM8bLyKvA/gOgDoAvQB+mnzdAEABHADwA1XtcW8sx/PGz5w506zPnTvXrC9evLjgsV7f9IYbbjDrZ86cMevWWn1vXba3z/hnn31m1r3zr1v9Zm8Pc2//9erqarPe0tKSWps6dao51jv2wVvP7q1Jt+633t5ec+ySJUvMetp5492DalR13TgXv+KNI6LKwsNliYJg2ImCYNiJgmDYiYJg2ImCqKgtm2+77TZz/PPPP59au+aaa8yx06dPN+vWUkzAXm75xRdfmGO95bdeC8lrQVmnwfZOBd3Z2WnW165da9ZbW+2joK1tmWfMSD3KGgCwcOFCs+7Zv39/as3bLrq/v9+se0tgvZam1fq76qqrzLHe7wu3bCYKjmEnCoJhJwqCYScKgmEnCoJhJwqCYScKoux9dqtfvWPHDnN8fX19as3rk3v1LKcO9k557PW6s6qtrU2t1dXVmWMfffRRs75y5Uqz/uSTT5p1a4ns6dOnzbGffvqpWbf66IC9LDnr8lpvaa/Xx7fGe8tnr732WrPOPjtRcAw7URAMO1EQDDtREAw7URAMO1EQDDtREGXts9fV1ekDDzyQWt+wYYM5ft++fak179TAXt3b/tfi9VytPjgAHDp0yKx7p3O21vJbp5kGgDlz5pj1NWvWmHVrW2TAXpPuPSY333xzprr1s3t9dO9+87Zk9ljnIPB+n6zzPhw+fBjDw8PssxNFxrATBcGwEwXBsBMFwbATBcGwEwXBsBMF4e7iWkwjIyPo6+tLrXv9ZmuNsLetsXfdXs/X6qt65/k+duyYWT948KBZ9+ZmrZf31ox757TfsmWLWW9vbzfrVp/d20bb64V75+u3tqv2fm5vTbnXC/fGW312r4dvbfFt3SfuM7uILBCRP4vIHhH5WER+lFw+U0S2icgnyUf7jP9ElKuJvIwfAfATVf02gNsA/FBEvg3gGQDbVXUxgO3J10RUodywq2qPqu5KPu8H0AlgHoDVADYl37YJgH1cJRHl6mu9QSciCwEsBfAXALNVtScpHQYwO2VMk4i0ikir9zcYEZXOhMMuIlMB/AHAj1X15Niajq6mGXdFjao2q2qjqjZmXTxARIWbUNhFZDJGg/5bVd2cXNwrIvVJvR5A+tvsRJQ7t/Umoz2CVwB0qurPx5S2AlgPYEPy8Q3vuoaHh9Hd3Z1a95bbdnV1pdZqamrMsd4plb02ztGjR1NrR44cMcdOmmTfzd7yWq/NYy0z9U5p7C3ltH5uAFiyZIlZHxwcTK157dDjx4+bde9+s+ZuteUAvzXnjfe2bLaWFp84ccIc29DQkFrr6OhIrU2kz34HgH8G0C4iu5PLnsVoyH8vIo8DOAjA3sibiHLlhl1V/wdA2hEA3y3udIioVHi4LFEQDDtREAw7URAMO1EQDDtREGVd4jo0NITdu3en1jdv3pxaA4DHHnssteadbtnb3tdbCmotM/X64F7P1Tuy0NsS2lre621V7R3b4G1l3dPTY9at6/fm5h2fkOUxy7p8NsvyWsDu4y9atMgc29vbW9Dt8pmdKAiGnSgIhp0oCIadKAiGnSgIhp0oCIadKIiybtksIplu7L777kutPf300+bYWbNmmXVv3bbVV/X6xV6f3Ouze/1m6/qtUxYDfp/dO4bAq1s/mzfWm7vHGm/1qifCe8y8U0lb69nb2trMsWvX2qvJVZVbNhNFxrATBcGwEwXBsBMFwbATBcGwEwXBsBMFUfY+u3Wecq83mcXdd99t1l944QWzbvXpa2trzbHeudm9PrzXZ/f6/BZrC23A78Nb+wAA9mM6MDBgjvXuF481d2+9ubeO33tMt23bZtY7OztTay0tLeZYD/vsRMEx7ERBMOxEQTDsREEw7ERBMOxEQTDsREG4fXYRWQDgNwBmA1AAzar6HyLyHIB/AXBhc/JnVfVt57rK19QvoxtvvNGsZ90bfv78+Wb9wIEDqTWvn7xv3z6zTt88aX32iWwSMQLgJ6q6S0SmAfhIRC4cMfALVf33Yk2SiEpnIvuz9wDoST7vF5FOAPNKPTEiKq6v9Te7iCwEsBTAX5KLnhKRNhF5VURmpIxpEpFWEWnNNFMiymTCYReRqQD+AODHqnoSwC8BfAtAA0af+X823jhVbVbVRlVtLMJ8iahAEwq7iEzGaNB/q6qbAUBVe1X1nKqeB/ArAMtKN00iysoNu4yeovMVAJ2q+vMxl9eP+bbvAego/vSIqFgm0npbDuC/AbQDuLBe8VkA6zD6El4BHADwg+TNPOu6LsnWG1ElSWu9faPOG09EPq5nJwqOYScKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKYiJnly2mowAOjvm6LrmsElXq3Cp1XgDnVqhizu3atEJZ17N/5cZFWiv13HSVOrdKnRfAuRWqXHPjy3iiIBh2oiDyDntzzrdvqdS5Veq8AM6tUGWZW65/sxNR+eT9zE5EZcKwEwWRS9hFZJWI/FVE9orIM3nMIY2IHBCRdhHZnff+dMkeen0i0jHmspkisk1EPkk+jrvHXk5ze05EupP7breI3J/T3BaIyJ9FZI+IfCwiP0ouz/W+M+ZVlvut7H+zi0gVgL8BWAGgC8BOAOtUdU9ZJ5JCRA4AaFTV3A/AEJG7AAwA+I2q/kNy2YsAjqnqhuQ/yhmq+q8VMrfnAAzkvY13sltR/dhtxgGsAfAocrzvjHmtRRnutzye2ZcB2Kuq+1V1GMDvAKzOYR4VT1XfB3DsootXA9iUfL4Jo78sZZcyt4qgqj2quiv5vB/AhW3Gc73vjHmVRR5hnwfg0Jivu1BZ+70rgD+KyEci0pT3ZMYxe8w2W4cBzM5zMuNwt/Eup4u2Ga+Y+66Q7c+z4ht0X7VcVf8JwH0Afpi8XK1IOvo3WCX1Tie0jXe5jLPN+JfyvO8K3f48qzzC3g1gwZiv5yeXVQRV7U4+9gHYgsrbirr3wg66yce+nOfzpUraxnu8bcZRAfddntuf5xH2nQAWi8giEZkC4PsAtuYwj68QkZrkjROISA2Alai8rai3AliffL4ewBs5zuXvVMo23mnbjCPn+y737c9Vtez/ANyP0Xfk9wH4tzzmkDKv6wD8b/Lv47znBuB1jL6sO4vR9zYeB3A1gO0APgHwJwAzK2hu/4nRrb3bMBqs+pzmthyjL9HbAOxO/t2f931nzKss9xsPlyUKgm/QEQXBsBMFwbATBcGwEwXBsBMFwbATBcGwEwXx//5fN5ZQVuVBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ5CKv-ojBIl"
      },
      "source": [
        "Using these improved images lets compile our model using an adaptive optimizer to learn faster and a categorical loss function to differentiate between the the various classes we are trying to classify. Since this is a very simple dataset we will only train for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6yg1_S0jBIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6b0e10-9b9c-4971-ba5d-c10b6a8ef1d5"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "# test the model on the test data\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.5861 - accuracy: 0.7942\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3724 - accuracy: 0.8658\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3213 - accuracy: 0.8801\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2926 - accuracy: 0.8914\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2804 - accuracy: 0.8958\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3839 - accuracy: 0.8629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38393157720565796, 0.8629000186920166]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJMsvSB-1UY"
      },
      "source": [
        "Once it's done training -- you should see an accuracy value at the end of the final epoch. It might look something like 0.8658. This tells you that your neural network is about 89% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 89% of the time. But how would it work with unseen data? That's why we have the test images. We can call ```model.evaluate```, and pass in the two sets, and it will report back the loss for each. This should reach about .8747 or thereabouts, showing about 87% accuracy. Not Bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rquQqIx4AaGR"
      },
      "source": [
        "But what did it actually learn? If we inference on the model using ```model.predict``` we get out the following list of values. **What does it represent?**\n",
        "\n",
        "*A hint: trying running ```print(test_labels[0])```*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyEIki0z_hAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd44c36-b178-4366-9ba4-2367e1137bc6"
      },
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.1455151e-04 5.2506350e-11 9.9338704e-01 1.4964666e-09 2.3300333e-03\n",
            " 1.8277138e-12 4.1684643e-03 2.3116498e-16 1.2072608e-10 5.0830705e-13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XKbbzVYnS_5",
        "outputId": "0344a396-f82a-4ea1-f908-3951e588c2e1"
      },
      "source": [
        "np.argmax(classifications[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B7ChU4XqJEt",
        "outputId": "7e25efea-25be-421f-d814-9b63a1344c91"
      },
      "source": [
        "test_labels[1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1DwozemsRC",
        "outputId": "77a9c9a9-8a51-4972-8445-0b910e05ca55"
      },
      "source": [
        "# Labels\r\n",
        "np.unique(test_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQSIfDSOWv6"
      },
      "source": [
        "Let's now look at the layers in your model. What happens if you double the number of neurons in the dense layer. What different results do you get for loss, training time etc? Why do you think that's the case? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSZSwV5UObQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cbef1f-71e6-4504-93a3-d6cfac5ea896"
      },
      "source": [
        "%%time\n",
        "NUMBER_OF_NEURONS = 512 * 2\n",
        "\n",
        "# define the new model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(NUMBER_OF_NEURONS, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "# compile fit and evaluate the model again\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.5888 - accuracy: 0.7928\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3625 - accuracy: 0.8679\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.3250 - accuracy: 0.8789\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.2957 - accuracy: 0.8885\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.2845 - accuracy: 0.8955\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3271 - accuracy: 0.8817\n",
            "CPU times: user 1min 52s, sys: 4.89 s, total: 1min 57s\n",
            "Wall time: 1min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6UB0qFAsLLW"
      },
      "source": [
        "**Dense Layer with double neurons size conclusions** </br>\r\n",
        "\r\n",
        "> The **loss decreased** a bit, from loss: *0.3839* to loss: *0.3271*\r\n",
        "The **accuracy improved** a bit as well, from: *0.8629* to **0.8817**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0lF5MuvSuZF"
      },
      "source": [
        "Consider the effects of additional layers in the network instead of simply more neurons to the same layer. First update the model to add an additional dense layer into the model between the two existing Dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_tUzB9XjBIn"
      },
      "source": [
        "YOUR_NEW_LAYER = tf.keras.layers.Dense(1024, activation=tf.nn.relu)\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    YOUR_NEW_LAYER,\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsX946WIjBIn"
      },
      "source": [
        "Lets then compile, fit, and evaluate our model. What happens to the error? How does this compare to the original model and the model with double the number of neurons?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1YPa6UhS8Es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cc8cf7-638e-48a3-e749-3bb6b5f712fc"
      },
      "source": [
        "%%time\n",
        "# compile fit and evaluate the model again\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5705 - accuracy: 0.7924\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.3609 - accuracy: 0.8650\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.3221 - accuracy: 0.8811\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2986 - accuracy: 0.8884\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2705 - accuracy: 0.8987\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.3500 - accuracy: 0.8704\n",
            "CPU times: user 3min 10s, sys: 8.48 s, total: 3min 18s\n",
            "Wall time: 2min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7UWL3DEug10"
      },
      "source": [
        "**Adding another Dense Layer with double neurons size conclusions** </br>\r\n",
        "\r\n",
        "> It performed not better than the previous model, I'd expect it to perform a bit better, since I added another Dense Layer.\r\n",
        "Also the time increased double, from 1:15 to 2:04\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3vVkOgCDGZ"
      },
      "source": [
        "Before you trained, you normalized the data. What would be the impact of removing that? To see it for yourself fill in the following lines of code to get a non-normalized set of data and then re-fit and evaluate the model using this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDqNAqrpCNg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e21b4a7-976b-4da0-d88a-f6078673c1eb"
      },
      "source": [
        "%%time\n",
        "# get new non-normalized mnist data\n",
        "training_images_non = training_images * 255\n",
        "test_images_non = test_images * 255\n",
        "\n",
        "# re-compile, re-fit and re-evaluate\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    YOUR_NEW_LAYER,\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images_non, training_labels, epochs=5)\n",
        "model.evaluate(test_images_non, test_labels)\n",
        "classifications = model.predict(test_images_non)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 4.9054 - accuracy: 0.6848\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.5133 - accuracy: 0.8211\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4801 - accuracy: 0.8370\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.4576 - accuracy: 0.8454\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4261 - accuracy: 0.8539\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4579 - accuracy: 0.8512\n",
            "CPU times: user 3min 11s, sys: 8.77 s, total: 3min 20s\n",
            "Wall time: 2min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_SwvYKJvvym"
      },
      "source": [
        "**non-normalized data conclusions** </br>\r\n",
        "\r\n",
        "> Obviously it performed as expected. Working with RGB non-normalized images doesn't improve the training. It's better to work with values on the range [0, 1]\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7W2PT66ZBHQ"
      },
      "source": [
        "Sometimes if you set the training for too many epochs you may find that training stops improving and you wish you could quit early. Good news, you can! TensorFlow has a function called ```Callbacks``` which can check the results from each epoch. Modify this callback function to make sure it exits training early but not before reaching at least the second epoch!\n",
        "\n",
        "*A hint: logs.get(METRIC_NAME) will return the value of METRIC_NAME at the current step*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkaEHHgqZbYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771f5ba0-fdcc-4665-8285-ef386891dd06"
      },
      "source": [
        "%%time\n",
        "# define and instantiate your custom Callback\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if( logs.get('accuracy') > 0.86): # it reached before 0.8704, so I think 0.86 is enough\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "\n",
        "# re-compile, re-fit and re-evaluate\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                            tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                            YOUR_NEW_LAYER,\n",
        "                            tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "      loss = 'sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5753 - accuracy: 0.7927\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.3694 - accuracy: 0.8629\n",
            "CPU times: user 1min 13s, sys: 3.28 s, total: 1min 17s\n",
            "Wall time: 48.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIQesAvvyqvP"
      },
      "source": [
        "Not sure why it reached accuracy of 0.8629 just on the second Epoch. But the Callback worked, it exits correctly."
      ]
    }
  ]
}